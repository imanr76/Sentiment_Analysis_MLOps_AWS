from dotenv import load_dotenvimport boto3import sagemakerfrom sagemaker.pytorch.estimator import PyTorch as PytorchEstimatorimport osdef setup_sagemaker(local = True):    """    Sets up the sagemaker and boto3 sessions required for running the processing job.    Parameters    ----------    local : boolean        Whether the script is running locally or inside sagemaker notebooks.    Returns    -------    role : str, obj        ARN role for the sagemaker session.    bucket : str        The default bucket name for the sagemaker session.    region : str        Teh region of the sgaemaker and boto3 session.    boto3_session : obj    sagemaker_Sess : obj    """    # IF running the script locally    if local:        load_dotenv()        role = os.getenv("ROLE")        profile_name  = os.getenv("AWS_PROFILE")        boto3_session = boto3.session.Session(profile_name = profile_name)        sagemaker_Sess = sagemaker.Session(boto_session = boto3_session)    # If running the code from a sagemaker notebook    else:        boto3_session = boto3.session.Session()        sagemaker_Sess = sagemaker.Session()        role = sagemaker_Sess.get_execution_role()        region = sagemaker_Sess.boto_region_name    bucket = sagemaker_Sess.default_bucket()        return role, bucket, region, boto3_session, sagemaker_Sessdef run_training_job(embed_dim = 20, lstm_size = 20, bidirectional = True,                num_layers = 1, dropout = 0.0, learning_rate = 0.001,                epochs = 100, threshold = 0.5, batch_size = 32, train_instance_type = "ml.m5.xlarge",                train_instance_count = 1, local = True, session_info = None,):    """    Runs a training job on AWS to train the model. The model artifacts are saved on S3.         NOTE: To change the location at which the model artifacts are saved, change output_path in the     PytorchEstimator object.        Parameters    ----------    embed_dim : int, optional        Size of the embedding vector for each token. The default is 20.    lstm_size : int, optional        Size of the lstm output. The default is 20.    bidirectional : boolean, optional        Whether to run a bidirectional LSTM. The default is True.    num_layers : int, optional        Number of LSTM layers. The default is 1.    dropout : float, optional        LSTM dropout. The default is 0.    learning_rate : float, optional        Learning rate for trianing the model. The default is 0.001.    epochs : int, optional        Number of epochs to run. The default is 5.    threshold : float, optional        Setting the threshold for positive and negative labels. The default is 0.5.    batch_size : int, optional        The batch size to be used during model training. The default is 32.        train_instance_type : str, optional        The type of VM nodes to use for training job. The default is "ml.m5.xlarge".    train_instance_count : int, optional        The number of VM nodes to use for training job. The default is 1.    local : boolean, optional        Whether the script is run on a local machine (True) or in a sagemaker notebook environment. The default is True.    session_info : obj        Information about the boto3 and sagemaker sessions.        Returns    -------    str        training job name.    """    if not session_info:           role, bucket, region, boto3_session, sagemaker_Sess = setup_sagemaker(local)    else:        role, bucket, region, boto3_session, sagemaker_Sess = session_info        hyperparameters = {                        "embed_dim" : embed_dim,                        "lstm_size" : lstm_size,                        "bidirectional" : bidirectional,                        "num_layers" : num_layers,                        "dropout" : dropout,                        "learning_rate" : learning_rate,                        "epochs" : epochs,                        "threshold" : threshold,                        "batch_size" : batch_size                        }        data_channels = {"train" : "s3://" + bucket + "/data/training/",                     "validation" : "s3://" + bucket + "/data/validation/",                     "test" : "s3://" + bucket + "/data/test/",                     "vocabulary" : "s3://" + bucket + "/models/vocabulary/"}            estimator = PytorchEstimator(                                entry_point = "./src/training.py",                                framework_version = "1.13",                                py_version = "py39",                                role = role,                                instance_count = train_instance_count,                                instance_type = train_instance_type,                                hyperparameters = hyperparameters,                                input_mode = 'File',                                output_path = "s3://" + bucket + "/models/",                                sagemaker_session = sagemaker_Sess                                )            estimator.fit(inputs = data_channels, logs = True)        return estimator.latest_training_job.job_name    #------------------------------------------------------------------------------# Running the script directlyif __name__ == "__main__":        # Size of the embedding vector for each token    embed_dim = 20    # Size of the lstm output    lstm_size = 20    # Whether to run a bidirectional LSTM    bidirectional = True    # Number of LSTM layers    num_layers = 1    # LSTM dropout    dropout = 0    # Learning rate for trianing the model    learning_rate = 0.001    # Number of epochs to run    epochs = 5    # Setting the threshold for positive and negative labels    threshold = 0.5    # The batch size to be used during model training.    batch_size = 32    # The type of VM nodes to use for training job.    train_instance_type = "ml.m5.xlarge"    # The number of VM nodes to use for training job.    train_instance_count = 1    #  Whether the script is run on a local machine (True) or in a sagemaker notebook environment.    local = True        run_training_job(embed_dim, lstm_size, bidirectional, num_layers, dropout, learning_rate,                    epochs, threshold, batch_size, train_instance_type, train_instance_count, local)